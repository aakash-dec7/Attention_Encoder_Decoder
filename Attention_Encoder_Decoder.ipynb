{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-Parameters\n",
    "MAX_LENGTH = 25\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.3\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 0.001\n",
    "CLIP = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>English words/sentences</th>\n",
       "      <th>French words/sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>go</td>\n",
       "      <td>ve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>go</td>\n",
       "      <td>vete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>go</td>\n",
       "      <td>vaya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>go</td>\n",
       "      <td>vayase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>hi</td>\n",
       "      <td>hola</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 English words/sentences French words/sentences\n",
       "0           0                      go                     ve\n",
       "1           1                      go                   vete\n",
       "2           2                      go                   vaya\n",
       "3           3                      go                 vayase\n",
       "4           4                      hi                   hola"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"eng_spn.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = df[\"English words/sentences\"]\n",
    "target_data = df[\"Spanish words/sentences\"].apply(lambda x: \"<sos> \" + x + \" <eos>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize dataset\n",
    "input_tokenizer = Tokenizer()\n",
    "input_tokenizer.fit_on_texts(input_data)\n",
    "input_sequences = input_tokenizer.texts_to_sequences(input_data)\n",
    "\n",
    "target_tokenizer = Tokenizer()\n",
    "target_tokenizer.fit_on_texts(target_data)\n",
    "target_sequences = target_tokenizer.texts_to_sequences(target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences\n",
    "padded_input_sequences = pad_sequences(\n",
    "    input_sequences, maxlen=MAX_LENGTH, padding=\"post\"\n",
    ")\n",
    "\n",
    "padded_target_sequences = pad_sequences(\n",
    "    target_sequences, maxlen=MAX_LENGTH, padding=\"post\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab size\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "target_vocab_size = len(target_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pytorch tensors\n",
    "input_tensor = torch.tensor(padded_input_sequences, dtype=torch.long)\n",
    "target_tensor = torch.tensor(padded_target_sequences, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "dataloader = DataLoader(\n",
    "    TensorDataset(input_tensor, target_tensor), batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convert tokens to vectors\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # Pass the embedded vector into LSTM layer\n",
    "        lstm_output, (hidden, cell) = self.lstm(embedded)\n",
    "\n",
    "        return lstm_output, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bahdanau Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "\n",
    "        # Linear layer to transform query(Q), key(K) and value(V)\n",
    "        self.w_Q = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.w_K = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.w_V = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "\n",
    "        # Extract the last hidden state\n",
    "        hidden = hidden[-1]\n",
    "        hidden = hidden.unsqueeze(1)\n",
    "\n",
    "        # Query from decoder\n",
    "        Q = self.w_Q(hidden)\n",
    "\n",
    "        # Key from encoder\n",
    "        K = self.w_K(encoder_outputs)\n",
    "\n",
    "        # Attention scores\n",
    "        attention_scores = self.w_V(torch.tanh(Q + K))\n",
    "\n",
    "        # Attention weights\n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        attention_weights = attention_weights.transpose(1, 2)\n",
    "\n",
    "        # Context vector\n",
    "        context_vector = torch.bmm(attention_weights, encoder_outputs)\n",
    "\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Attention layer\n",
    "        self.attention = BahdanauAttention(hidden_dim)\n",
    "\n",
    "        # Linear layer\n",
    "        self.fc = nn.Linear(hidden_dim * 2, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell, encoder_outputs):\n",
    "        # Add batch dimension\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        # Convert tokens to vectors\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # Pass the embedded vector into LSTM layer\n",
    "        lstm_output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "\n",
    "        # Calculate context vector\n",
    "        context_vector = self.attention(hidden, encoder_outputs)\n",
    "\n",
    "        # Concatenate lstm_output and context_vector\n",
    "        concatenated = torch.cat((lstm_output, context_vector), dim=2)\n",
    "\n",
    "        # Generate predictions for the next token\n",
    "        prediction = self.fc(concatenated)\n",
    "\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_vocab_size,\n",
    "        target_vocab_size,\n",
    "        embed_dim,\n",
    "        hidden_dim,\n",
    "        num_layers,\n",
    "        dropout,\n",
    "    ):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(\n",
    "            input_vocab_size, embed_dim, hidden_dim, num_layers, dropout\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = Decoder(\n",
    "            target_vocab_size, embed_dim, hidden_dim, num_layers, dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        batch_size, max_length = target.size()\n",
    "        target_vocab_size = self.decoder.fc.out_features\n",
    "\n",
    "        # Tensor to store outputs for all time steps\n",
    "        outputs = torch.zeros(batch_size, max_length, target_vocab_size)\n",
    "\n",
    "        # Get encoder outputs, hidden and cell states from the encoder\n",
    "        encoder_outputs, hidden, cell = self.encoder(input)\n",
    "\n",
    "        # Start decoding with the first target token\n",
    "        target_input_token = target[:, 0]\n",
    "\n",
    "        for t in range(1, max_length):\n",
    "            decoder_output, hidden, cell = self.decoder(\n",
    "                target_input_token, hidden, cell, encoder_outputs\n",
    "            )\n",
    "            outputs[:, t, :] = decoder_output.squeeze(1)\n",
    "\n",
    "            target_input_token = target[:, t]\n",
    "            hidden = hidden\n",
    "            cell = cell\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = Seq2Seq(\n",
    "    input_vocab_size,\n",
    "    target_vocab_size,\n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    NUM_LAYERS,\n",
    "    DROPOUT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint\n",
    "\n",
    "\n",
    "# Save model function\n",
    "def save_checkpoint(epoch, model, filename=\"checkpoint.pth\"):\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "        },\n",
    "        filename,\n",
    "    )\n",
    "\n",
    "\n",
    "# Load model function\n",
    "def load_checkpoint(model, filename):\n",
    "    checkpoint = torch.load(filename)\n",
    "\n",
    "    start_epoch = checkpoint[\"epoch\"]\n",
    "\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    return start_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found, starting training from scratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aks7d\\AppData\\Local\\Temp\\ipykernel_16812\\3896224334.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename)\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "try:\n",
    "    start_epoch = load_checkpoint(model, filename=\"checkpoint.pth\")\n",
    "    print(f\"Resuming training from epoch: {start_epoch}\")\n",
    "except FileNotFoundError:\n",
    "    start_epoch = 1\n",
    "    print(f\"No checkpoint found, starting training from scratch...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Adam optimizer and Loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train function\n",
    "def train(model, optimizer, criterion, dataloader, epochs=NUM_EPOCHS):\n",
    "\n",
    "    model.train()  # Set model to Training mode\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for epoch in range(start_epoch, epochs + 1):\n",
    "        epoch_loss = 0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch}/{epochs}\")\n",
    "\n",
    "        for input, target in progress_bar:\n",
    "\n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(input, target)\n",
    "\n",
    "            # Reshape input and target to calculate loss\n",
    "            output = output[:, 1:].reshape(-1, output.shape[2])\n",
    "            target = target[:, 1:].reshape(-1)\n",
    "\n",
    "            # Compute loss and backpropagation\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip gradients to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "\n",
    "            # Update model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        total_loss += epoch_loss\n",
    "\n",
    "        progress_bar.close()\n",
    "\n",
    "        save_checkpoint(epoch, model)\n",
    "\n",
    "    print(f\"Total Loss: {total_loss/len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 157/157 [02:19<00:00,  1.13it/s, loss=3.69]\n",
      "Epoch 2/50: 100%|██████████| 157/157 [02:17<00:00,  1.14it/s, loss=2.91]\n",
      "Epoch 3/50: 100%|██████████| 157/157 [02:19<00:00,  1.12it/s, loss=2.3] \n",
      "Epoch 4/50: 100%|██████████| 157/157 [02:14<00:00,  1.17it/s, loss=1.82]\n",
      "Epoch 5/50: 100%|██████████| 157/157 [02:16<00:00,  1.15it/s, loss=1.17]\n",
      "Epoch 6/50: 100%|██████████| 157/157 [02:15<00:00,  1.16it/s, loss=0.926]\n",
      "Epoch 7/50: 100%|██████████| 157/157 [02:14<00:00,  1.17it/s, loss=0.843]\n",
      "Epoch 8/50: 100%|██████████| 157/157 [02:14<00:00,  1.17it/s, loss=0.729]\n",
      "Epoch 9/50: 100%|██████████| 157/157 [02:13<00:00,  1.18it/s, loss=1.03] \n",
      "Epoch 10/50: 100%|██████████| 157/157 [02:11<00:00,  1.19it/s, loss=0.734]\n",
      "Epoch 11/50: 100%|██████████| 157/157 [02:12<00:00,  1.18it/s, loss=0.715]\n",
      "Epoch 12/50: 100%|██████████| 157/157 [02:12<00:00,  1.19it/s, loss=0.65] \n",
      "Epoch 13/50: 100%|██████████| 157/157 [02:11<00:00,  1.19it/s, loss=0.48] \n",
      "Epoch 14/50: 100%|██████████| 157/157 [02:14<00:00,  1.17it/s, loss=0.567]\n",
      "Epoch 15/50: 100%|██████████| 157/157 [02:16<00:00,  1.15it/s, loss=0.54] \n",
      "Epoch 16/50: 100%|██████████| 157/157 [02:14<00:00,  1.17it/s, loss=0.665]\n",
      "Epoch 17/50: 100%|██████████| 157/157 [02:15<00:00,  1.16it/s, loss=0.53] \n",
      "Epoch 18/50: 100%|██████████| 157/157 [02:14<00:00,  1.17it/s, loss=0.298]\n",
      "Epoch 19/50: 100%|██████████| 157/157 [02:17<00:00,  1.15it/s, loss=0.347]\n",
      "Epoch 20/50: 100%|██████████| 157/157 [02:16<00:00,  1.15it/s, loss=0.448]\n",
      "Epoch 21/50: 100%|██████████| 157/157 [02:17<00:00,  1.14it/s, loss=0.456]\n",
      "Epoch 22/50: 100%|██████████| 157/157 [02:18<00:00,  1.14it/s, loss=0.464]\n",
      "Epoch 23/50: 100%|██████████| 157/157 [02:17<00:00,  1.14it/s, loss=0.618]\n",
      "Epoch 24/50: 100%|██████████| 157/157 [02:17<00:00,  1.15it/s, loss=0.593]\n",
      "Epoch 25/50: 100%|██████████| 157/157 [02:17<00:00,  1.14it/s, loss=0.378]\n",
      "Epoch 26/50: 100%|██████████| 157/157 [02:17<00:00,  1.14it/s, loss=0.339]\n",
      "Epoch 27/50: 100%|██████████| 157/157 [02:16<00:00,  1.15it/s, loss=0.435]\n",
      "Epoch 28/50: 100%|██████████| 157/157 [02:17<00:00,  1.14it/s, loss=0.346]\n",
      "Epoch 29/50: 100%|██████████| 157/157 [02:16<00:00,  1.15it/s, loss=0.195]\n",
      "Epoch 30/50: 100%|██████████| 157/157 [02:17<00:00,  1.14it/s, loss=0.311]\n",
      "Epoch 31/50: 100%|██████████| 157/157 [02:12<00:00,  1.18it/s, loss=0.359]\n",
      "Epoch 32/50: 100%|██████████| 157/157 [02:10<00:00,  1.20it/s, loss=0.408]\n",
      "Epoch 33/50: 100%|██████████| 157/157 [02:11<00:00,  1.20it/s, loss=0.26] \n",
      "Epoch 34/50: 100%|██████████| 157/157 [02:10<00:00,  1.20it/s, loss=0.224]\n",
      "Epoch 35/50: 100%|██████████| 157/157 [02:10<00:00,  1.20it/s, loss=0.198]\n",
      "Epoch 36/50: 100%|██████████| 157/157 [02:10<00:00,  1.20it/s, loss=0.256]\n",
      "Epoch 37/50: 100%|██████████| 157/157 [02:10<00:00,  1.20it/s, loss=0.58] \n",
      "Epoch 38/50: 100%|██████████| 157/157 [02:11<00:00,  1.20it/s, loss=0.203]\n",
      "Epoch 39/50: 100%|██████████| 157/157 [02:10<00:00,  1.20it/s, loss=0.296]\n",
      "Epoch 40/50: 100%|██████████| 157/157 [02:10<00:00,  1.20it/s, loss=0.417]\n",
      "Epoch 41/50: 100%|██████████| 157/157 [02:11<00:00,  1.20it/s, loss=0.24] \n",
      "Epoch 42/50: 100%|██████████| 157/157 [02:10<00:00,  1.20it/s, loss=0.304]\n",
      "Epoch 43/50: 100%|██████████| 157/157 [02:10<00:00,  1.20it/s, loss=0.319]\n",
      "Epoch 44/50: 100%|██████████| 157/157 [02:11<00:00,  1.20it/s, loss=0.283]\n",
      "Epoch 45/50: 100%|██████████| 157/157 [02:10<00:00,  1.20it/s, loss=0.371]\n",
      "Epoch 46/50: 100%|██████████| 157/157 [02:10<00:00,  1.20it/s, loss=0.357]\n",
      "Epoch 47/50: 100%|██████████| 157/157 [02:10<00:00,  1.20it/s, loss=0.451]\n",
      "Epoch 48/50: 100%|██████████| 157/157 [02:10<00:00,  1.20it/s, loss=0.199]\n",
      "Epoch 49/50: 100%|██████████| 157/157 [02:10<00:00,  1.21it/s, loss=0.204]\n",
      "Epoch 50/50: 100%|██████████| 157/157 [02:10<00:00,  1.20it/s, loss=0.305]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss: 30.547225832274766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "train(model, optimizer, criterion, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU score implementation (token-level)\n",
    "\n",
    "\n",
    "def compute_bleu(reference, candidate, max_n=4, weights=None):\n",
    "    if weights is None:\n",
    "        weights = [1 / max_n] * max_n\n",
    "\n",
    "    # No need to split, as reference and candidate are already tokenized lists\n",
    "    reference_tokens = reference\n",
    "    candidate_tokens = candidate\n",
    "\n",
    "    precisions = []\n",
    "    for n in range(1, max_n + 1):\n",
    "        # Extract n-grams for reference and candidate\n",
    "        ref_ngrams = Counter(\n",
    "            [\n",
    "                tuple(reference_tokens[i : i + n])\n",
    "                for i in range(len(reference_tokens) - n + 1)\n",
    "            ]\n",
    "        )\n",
    "        cand_ngrams = Counter(\n",
    "            [\n",
    "                tuple(candidate_tokens[i : i + n])\n",
    "                for i in range(len(candidate_tokens) - n + 1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        match_count = sum(min(ref_ngrams[ng], cand_ngrams[ng]) for ng in cand_ngrams)\n",
    "        total_count = max(len(candidate_tokens) - n + 1, 1)\n",
    "        precisions.append(match_count / total_count if total_count > 0 else 0)\n",
    "\n",
    "    reference_length = len(reference_tokens)\n",
    "    candidate_length = len(candidate_tokens)\n",
    "    brevity_penalty = (\n",
    "        math.exp(1 - reference_length / candidate_length)\n",
    "        if candidate_length < reference_length\n",
    "        else 1\n",
    "    )\n",
    "\n",
    "    bleu_score = brevity_penalty * math.exp(\n",
    "        sum(w * math.log(p) for w, p in zip(weights, precisions) if p > 0)\n",
    "    )\n",
    "\n",
    "    return bleu_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, compute_bleu, max_n=4, weights=None):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    total_bleu_score = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():  # No need to compute gradients during evaluation\n",
    "        for input, target in dataloader:\n",
    "\n",
    "            # Forward pass (get predictions)\n",
    "            output = model(input, target)\n",
    "\n",
    "            # Reshape output and target for comparison\n",
    "            output = output[:, 1:].argmax(\n",
    "                dim=-1\n",
    "            )  # Predicted tokens (without <start> token)\n",
    "            target = target[:, 1:]  # Ignore <start> token in the reference\n",
    "\n",
    "            # Convert target and predicted tokens to lists (detach from GPU if necessary)\n",
    "            target_tokens = target.cpu().tolist()\n",
    "            predicted_tokens = output.cpu().tolist()\n",
    "\n",
    "            # Calculate BLEU score for the current batch\n",
    "            batch_bleu_score = 0\n",
    "            for ref, pred in zip(target_tokens, predicted_tokens):\n",
    "                batch_bleu_score += compute_bleu(\n",
    "                    reference=ref, candidate=pred, max_n=max_n, weights=weights\n",
    "                )\n",
    "\n",
    "            # Accumulate BLEU scores and total samples\n",
    "            total_bleu_score += batch_bleu_score\n",
    "            total_samples += len(target_tokens)\n",
    "\n",
    "    avg_bleu_score = total_bleu_score / total_samples\n",
    "\n",
    "    return avg_bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17288777905138453"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, dataloader, compute_bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    model, input_text, input_tokenizer, target_tokenizer, max_length=MAX_LENGTH\n",
    "):\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Convert text to sequence\n",
    "    input_sequence = input_tokenizer.texts_to_sequences([input_text])\n",
    "\n",
    "    # Apply padding\n",
    "    padded_input_sequence = pad_sequences(\n",
    "        input_sequence, maxlen=max_length, padding=\"post\"\n",
    "    )\n",
    "\n",
    "    # Convert to torch tensor\n",
    "    input_tensor = torch.tensor(padded_input_sequence, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden, cell = model.encoder(input_tensor)\n",
    "\n",
    "    sos_token = target_tokenizer.word_index[\"sos\"]\n",
    "\n",
    "    # Start prediction with \"sos\" token\n",
    "    x_input = torch.tensor([sos_token], dtype=torch.long)\n",
    "\n",
    "    # List to store predicted tokens\n",
    "    translated_sentence = []\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            prediction, hidden, cell = model.decoder(\n",
    "                x_input, hidden, cell, encoder_outputs\n",
    "            )\n",
    "\n",
    "        predicted_token = prediction.argmax(-1).item()\n",
    "\n",
    "        # Stop prediction if \"eos\" is predicted\n",
    "        if predicted_token == target_tokenizer.word_index[\"eos\"]:\n",
    "            break\n",
    "\n",
    "        translated_sentence.append(predicted_token)\n",
    "\n",
    "        # Update x_input for next time step\n",
    "        x_input = torch.tensor([predicted_token], dtype=torch.long)\n",
    "\n",
    "    # Convert tokens back to text\n",
    "    translated_sentence = target_tokenizer.sequences_to_texts([translated_sentence])[0]\n",
    "\n",
    "    return translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: im thirtyfour\n",
      "Translated Sentence: tengo treinta y cuatro anos\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "input_sentence = \"im thirtyfour\"\n",
    "translated_sentence = predict(model, input_sentence, input_tokenizer, target_tokenizer)\n",
    "print(f\"Input Sentence: {input_sentence}\")\n",
    "print(f\"Translated Sentence: {translated_sentence}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
